# Project Bergeron

The goal of this project is to protect models against both natural language adversarial attacks and its own bias toward mis-alignment.  This is done through the usage of one or more overseer models that check the model's input and output in a context where they are less exposed to potential threats.

This can be thought of attaching a conscience to these models to help guide them toward aligned responses.

## Documentation

The documentation for this project can be found both inline with the code and in the [docs](docs) folder.  Additionally, the [researchNotes](docs/researchNotes) folder provides additional context to the general overview of the project's structure, sources, and inspirations.  A complete outline of the project can be found in the [projectOutline.md](docs/researchNotes/projectOutline.md) file.